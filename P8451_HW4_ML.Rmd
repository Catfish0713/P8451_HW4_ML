---
title: "P8451_HW4_ML"
author: "Ruixi Li"
date: "2024-02-07"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


# Part I: Implementing a Simple Prediction Pipeline

The New York City Department of Health administered a questionnaire on general health and physical activity among residents. Using the dataset class4_p1.csv, fit and evaluate two prediction models using **linear regression**. The aim of the models are to predict the number of days in a month an individual reported having good physical health (feature name: **healthydays**). A codebook is provided so you can look-up the meaning and values of each feature in the dataset. (Note the codebook lists information on features that are not included in your dataset).

Your analytic pipeline should include the following:

## 1. Data Cleaning

```{r load_packages, message=FALSE, warning=FALSE}
library(ggbiplot)
library(caret)
library(gtsummary)
library(MASS)
```


```{r data_cleaning, message=FALSE, warning=FALSE}
library(tidyverse)
# Read data
hw4data = read_csv("class4_p1.csv")
set.seed(123)

# View the datatypes and summary statistics of the whole dataset
skimr::skim(hw4data)

# Rename and datatype conversion
var.names = c("id", "hypertension", "diabetes", "asthma", "bmi", "tobacco", "alcohol", "pa_chores_min", "pa_walk_day", "pa_self", "diet", "agegrp", "gender", "race","nationality", "fincome", "healthyday")

hw4data = hw4data |>
  set_names(var.names) |>
  mutate(across(-c(5,8,9,17), function(x) as.factor(x)))

hw4data |> Amelia::missmap(main = "Missing values vs observed")

hw4data = hw4data|> 
  select(-id) |> 
  drop_na()

hw4data |> Amelia::missmap(main = "Missing values vs observed")
# it's necessary to make sure that directly drop NAs won't cause bias

```

## 2. Data partition

```{r data_partition}
# Creating balanced partitions in the data
train.index = createDataPartition(hw4data$healthyday, p = 0.7, list = FALSE)

train = hw4data[train.index, ]
test = hw4data[-train.index, ]

# I should check the independence of each variable
```

## 3. Fit two models

There are lots of methods to fit my models. I have thought about using lasso regression to do the auto-selection of predictors and handle multicollinearity. But lasso regression doesn't work well with categorical variables in terms of both feature selection and prediction accuracy. It needs One-hot encoding of the categorical data and ignore the grouping effect. However, group lasso may be a good alternative for robust feature selection although at the cost of prediction accuracy. (Lack of Robustness of Lasso and Group Lasso with Categorical Predictors: Impact of Coding
Strategy on Variable Selection and Prediction https://escholarship.org/content/qt40b200z6/qt40b200z6_noSplash_c3819f1c49cdc6380c6ae5b0ac0af41d.pdf?t=qaj586) 

Here, I would build the model1 based on my knowledge. I built my model2 by add predictors manually. 

```{r model_fit}
model1 = lm(healthyday ~ hypertension + agegrp + asthma + fincome, data = train)

# Conducting the bivariate analysis for model building

outcome <- "healthyday"
predictors <- c("hypertension", "diabetes", "asthma", "bmi", "tobacco", "alcohol", "pa_chores_min", "pa_walk_day", "pa_self", "diet", "agegrp", "gender", "race", "nationality", "fincome")

bivariate_results <- list()

for (predictor in predictors) {
  formula <- as.formula(paste(outcome, "~", predictor))
  model <- lm(formula, data=hw4data) 
  bivariate_results[[predictor]] <- summary(model)$coefficients
}


# Start with an intercept-only model
current_model <- lm(healthyday ~ 1, data=hw4data) 

# Iteratively add variables based on LRT
for (predictor in predictors) {
  new_model_formula <- update(formula(current_model), paste(". ~ . +", predictor))
  new_model <- lm(new_model_formula, data=hw4data) 
  
  # Perform Likelihood Ratio Test
  lr_test <- anova(current_model, new_model)
  
  # Extract the last p-value (corresponding to the addition of the new predictor)
  p_value <- lr_test$`Pr(>F)`[nrow(lr_test)]
  
  # Check if the p-value exists and is significant
  if (!is.na(p_value) && p_value < 0.05) {
    current_model <- new_model  # Update the model to include the new predictor
    cat("Added", predictor, "to the model\n")
  } else {
    cat(predictor, "not added\n")
  }
}
  
  
  # Final model after adding all significant predictors
  
model2 = current_model

```

```{r model_training}
# Avoid overfitting through cross validation
control = trainControl(method="repeatedcv", number=10, repeats=10, summaryFunction=defaultSummary)

# Train models
model1_train = train(healthyday ~ hypertension + agegrp + asthma + fincome, data = train, method = "lm", trControl = control)
model2_train = train(healthyday ~ hypertension + diabetes + asthma + alcohol + pa_walk_day + pa_self + diet + agegrp + race + fincome, data = train, method = "lm", trControl = control)

model1_train$finalModel |> tbl_regression()
model2_train$finalModel |> tbl_regression()
```

* model1: healthyday ~ hypertension + agegrp + asthma + fincome
* model2: healthyday ~ hypertension + diabetes + asthma + alcohol + pa_walk_day + 
    pa_self + diet + agegrp + race + fincome

## 4. Model Evaluation

```{r model_evaluation}
# Predictions for models
predictions1 = predict(model1_train, test)
predictions2 = predict(model2_train, test)

# Evaluation using RMSE and R2


rmse1 = RMSE(predictions1, test$healthyday)
rmse2 = RMSE(predictions2, test$healthyday)
r2_1=R2(predictions1, test$healthyday)
r2_2=R2(predictions2, test$healthyday)


```

* RMSE for model1(`r rmse1`) > model2(`r rmse2`),R2 for model1(`r r2_1`) < model2(`r r2_2`). So, model1 is the preferred prediction model using RMSE and R2 evaluation.

## 5. Implementing setting

My final model: 'healthyday ~ hypertension + agegrp + asthma + fincome' can be used to predict the number of days in a month an individual reported having good physical health through a combination of medical status(hypertension, asthma), age and the family income. Looking at the estimates of model 1, having no diseases, being in a younger age group and higher family income can increase healthydays. This insight can guide targeted health interventions(focus on chronic diseases like hypertension and asthma) and resource allocation(to poor people) to improve the general well-being of the population in NYC.




# Part II: Conducting an Unsupervised Analysis

Using the dataset from the Group assignment Part 3 (USArrests), identify clusters using hierarchical analysis.

## 6. Hierarchical clustering analysis

```{r hierarchical_clustering, message=FALSE}
library(factoextra)
library(cluster)

# Load data
data("USArrests")

# look at the structure and summary
skimr::skim(USArrests)

# Check means and SDs to determine if scaling is necessary
USArrests |>
  summarise_all(mean, na.rm = TRUE) |>
  print()

USArrests |>
  summarise_all(sd, na.rm = TRUE) |>
  print()
# need standardization

# Centering and Scaling
set.up.preprocess = preProcess(USArrests, method = c("center", "scale"))

# Output pre-processed values
transformed.vals = predict(set.up.preprocess, USArrests)




set.seed(123)

# Hierarchical clustering using Complete Linkage, 
clusters.hcut=hcut(transformed.vals, k=4, hc_func="hclust", hc_method="complete", hc_metric="euclidian")# k=2 or k=4

clusters.hcut$size

# Plot the obtained dendrogram
fviz_dend(clusters.hcut, rect=TRUE)
fviz_cluster(clusters.hcut)



```


## 7. Determine the optimal number of clusters

```{r gap_statistics}
# Plot gap statistic graph
gap_stat = clusGap(transformed.vals, FUN = hcut, hc_method="complete", K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

* Although the graph chose the optimal cluster number as 2, the gap statistics increases when moving k=3 to k=4, and constantly decreases when k becomes bigger,despite a slight decrease, indicating that k=4 is the optimal number.

## 8. Describe the composition of each cluster

```{r clster_estimate}

input.feature.vals=cbind(transformed.vals,cluster=clusters.hcut$cluster)

input.feature.vals |>
  group_by(cluster) |>
  summarise_all(mean) |>
  knitr::kable()
```

* Cluster 1: This cluster is marked by above-average values in all categories except UrbanPop, which is close to average (z-score of -0.832). It suggests states with higher crime rates and moderately smaller urban populations.

* Cluster 2: States in this cluster have all features above the average,especially Assault and Rape rates. This cluster might represent states with high crime rates and large urban populations.

* Cluster 3: The negative z-scores for Murder, Assault, and Rape indicate that these areas have lower than average rates of these crimes. The UrbanPop is slightly above average, meaning these areas might have moderately sized urban populations but high crime rates.

* Cluster 4: This cluster is characterized by below-average values for all features (Murder, Assault, UrbanPop, Rape), as indicated by the negative z-scores. This suggests that states falling into this cluster have lower than average crime rates and urban population.


## 9. Research Questions

* Research question: Is the level of utilization(represented by 'UrbanPop' feature in this dataset) associated with the crime rate(measured by 'Murder', 'Assualt' and 'Rape' features in this dataset) across state?

* Scientific consideration: We should figure out which linkage method to use and how many clusters to have based on the nature of data.

* Ethical consideration:We should avoid drawing conclusions that could stigmatize certain regions or communities.

## 10. Repeat analysis with different parameters

```{r}
clusters.hcut<-hcut(transformed.vals, k=5, hc_func="hclust", hc_method="single", hc_metric="euclidian")

clusters.hcut$size
fviz_dend(clusters.hcut, rect=TRUE)
fviz_cluster(clusters.hcut)

gap_stat <- clusGap(transformed.vals, FUN = hcut, hc_method="single", K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

input.feature.vals<-cbind(transformed.vals,cluster=clusters.hcut$cluster)

input.feature.vals %>%
  group_by(cluster) %>%
  summarise_all(mean)

transformed.vals|>
  ggplot(aes(y=Rape))+ geom_boxplot()

```

* Yes, the clusters changed. For my research question, I want to find tightly knit clusters and the data contains outliers(in Rape), so compared with 'single' I think 'complete' is better(using single linkage, some cluster has only one state). The gap statistic graph also shows an obvious 'elbow' at k=4(compared with using 'single' method). Changing set seeds didn't change the clusters a lot,indicating that the data has a strong inherent tighter clustering structure. 





